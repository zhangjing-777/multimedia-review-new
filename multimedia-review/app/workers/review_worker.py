"""
å®¡æ ¸ä»»åŠ¡å¤„ç†å™¨
è´Ÿè´£å¼‚æ­¥å¤„ç†å®¡æ ¸ä»»åŠ¡å’Œæ–‡ä»¶
"""

import asyncio
from typing import List, Dict
from celery import current_task
from sqlalchemy.orm import Session
from loguru import logger
import PyPDF2
from pdf2image import convert_from_path
from docx import Document
import cv2
import uuid
import os
import shutil
from datetime import datetime

from app.workers.celery_app import celery_app
from app.database import SessionLocal
from app.services import (
    TaskService, FileService, OCRService, 
    AIReviewService, QueueService
)
from app.models.task import TaskStatus, ReviewTask
from app.models.file import FileStatus, FileType, ReviewFile
from app.models.result import ReviewResult, ViolationResult, SourceType
from app.utils.file_utils import FileUtils


@celery_app.task(bind=True, name="process_review_task")
def process_review_task(self, task_id: str):
    """
    å¤„ç†å®¡æ ¸ä»»åŠ¡çš„ä¸»æµç¨‹ï¼ˆä¿®å¤é”™è¯¯failedçŠ¶æ€ï¼‰
    """
    db = SessionLocal()
    queue_service = QueueService()
    
    try:
        # ä½¿ç”¨åˆ†å¸ƒå¼é”ç¡®ä¿ä»»åŠ¡å”¯ä¸€æ€§
        with queue_service.task_lock(task_id):
            task_service = TaskService(db)
            file_service = FileService(db)
            
            # è·å–ä»»åŠ¡ä¿¡æ¯
            task = task_service.get_task_by_id(task_id)
            logger.info(f"ğŸ”’ å¼€å§‹å¤„ç†ä»»åŠ¡: {task.name} (å·²åŠ é”)")
            
            # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼Œé¿å…é‡å¤å¤„ç†
            if task.status not in [TaskStatus.PENDING, TaskStatus.PROCESSING]:
                logger.warning(f"ä»»åŠ¡ {task_id} çŠ¶æ€ä¸º {task.status}ï¼Œè·³è¿‡å¤„ç†")
                return {"status": "skipped", "reason": f"ä»»åŠ¡çŠ¶æ€: {task.status}"}
            
            # è·å–ä»»åŠ¡çš„æ‰€æœ‰æ–‡ä»¶
            files = task_service.get_task_files(task_id, status=FileStatus.PENDING)
            
            if not files:
                # ä¿®å¤ï¼šæ²¡æœ‰æ–‡ä»¶ä¸åº”è¯¥æ ‡è®°ä¸ºå¤±è´¥ï¼Œè€Œæ˜¯å®Œæˆ
                task_service.complete_task(task_id, success=True, error_message="æ²¡æœ‰å¾…å¤„ç†çš„æ–‡ä»¶")
                return {"status": "completed", "message": "æ²¡æœ‰å¾…å¤„ç†çš„æ–‡ä»¶"}
            
            # å°†æ‰€æœ‰æ–‡ä»¶æ·»åŠ åˆ°å¤„ç†é˜Ÿåˆ—
            for file_obj in files:
                queue_service.add_file_to_queue(
                    file_id=str(file_obj.id),
                    task_id=task_id,
                    file_type=file_obj.file_type.value
                )
            
            # æ›´æ–°ä»»åŠ¡è¿›åº¦
            queue_service.update_progress(
                task_id, 
                progress=10, 
                message=f"å·²å°†{len(files)}ä¸ªæ–‡ä»¶åŠ å…¥å¤„ç†é˜Ÿåˆ—"
            )
            
            return {
                "status": "processing",
                "message": f"ä»»åŠ¡å·²å¯åŠ¨ï¼Œ{len(files)}ä¸ªæ–‡ä»¶è¿›å…¥å¤„ç†é˜Ÿåˆ—"
            }
    
    except RuntimeError as e:
        # é”å†²çªï¼Œä»»åŠ¡å·²è¢«å…¶ä»–è¿›ç¨‹å¤„ç†
        logger.warning(f"ä»»åŠ¡é”å†²çª: {e}")
        return {"status": "skipped", "reason": "ä»»åŠ¡æ­£åœ¨è¢«å…¶ä»–è¿›ç¨‹å¤„ç†"}
    
    except Exception as e:
        logger.error(f"å¤„ç†ä»»åŠ¡å¤±è´¥ {task_id}: {e}")
        
        # ä¿®å¤ï¼šä¸è¦ç«‹å³æ ‡è®°ä»»åŠ¡ä¸ºå¤±è´¥ï¼Œåªè®°å½•é”™è¯¯
        # è®©æ–‡ä»¶å¤„ç†å®Œæˆåå†ç»Ÿä¸€åˆ¤æ–­ä»»åŠ¡çŠ¶æ€
        try:
            # åªæ›´æ–°é”™è¯¯ä¿¡æ¯ï¼Œä¸æ”¹å˜ä»»åŠ¡çŠ¶æ€
            task_service = TaskService(db)
            task = task_service.get_task_by_id(task_id)
            if task.status == TaskStatus.PROCESSING:
                # å¦‚æœä»»åŠ¡è¿˜åœ¨å¤„ç†ä¸­ï¼Œä¸æ”¹å˜çŠ¶æ€ï¼Œåªè®°å½•é”™è¯¯
                task.error_message = f"å¯åŠ¨è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜: {str(e)}"
                task.updated_at = datetime.utcnow()
                db.commit()
                logger.info(f"è®°å½•ä»»åŠ¡é”™è¯¯ä½†ä¿æŒå¤„ç†çŠ¶æ€: {task_id}")
            # å¦‚æœä»»åŠ¡ä¸åœ¨å¤„ç†ä¸­ï¼Œæ‰æ ‡è®°ä¸ºå¤±è´¥
            elif task.status == TaskStatus.PENDING:
                task_service.complete_task(task_id, success=False, error_message=str(e))
        except Exception as save_error:
            logger.error(f"ä¿å­˜ä»»åŠ¡é”™è¯¯ä¿¡æ¯æ—¶å‡ºé”™: {save_error}")
        
        return {"status": "error", "error": str(e)}
    
    finally:
        db.close()
 

@celery_app.task(bind=True, name="process_review_file") 
def process_review_file(self, file_id: str, task_id: str, file_type: str):
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶çš„å®¡æ ¸ï¼ˆå¸¦é”æœºåˆ¶ï¼‰
    """
    db = SessionLocal()
    queue_service = QueueService()
    
    try:
        # ä½¿ç”¨åˆ†å¸ƒå¼é”ç¡®ä¿æ–‡ä»¶å”¯ä¸€æ€§
        with queue_service.file_lock(file_id):
            file_service = FileService(db)
            
            # è·å–æ–‡ä»¶ä¿¡æ¯
            file_obj = file_service.get_file_by_id(file_id)
            logger.info(f"ğŸ”’ å¼€å§‹å¤„ç†æ–‡ä»¶: {file_obj.original_name} (å·²åŠ é”)")
            
            # æ£€æŸ¥æ–‡ä»¶çŠ¶æ€ï¼Œé¿å…é‡å¤å¤„ç†
            if file_obj.status not in [FileStatus.PENDING, FileStatus.PROCESSING]:
                logger.warning(f"æ–‡ä»¶ {file_id} çŠ¶æ€ä¸º {file_obj.status}ï¼Œè·³è¿‡å¤„ç†")
                return {"status": "skipped", "reason": f"æ–‡ä»¶çŠ¶æ€: {file_obj.status}"}
            
            # æ›´æ–°æ–‡ä»¶çŠ¶æ€ä¸ºå¤„ç†ä¸­
            file_service.update_file_status(file_id, FileStatus.PROCESSING, progress=0)
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹è¿›è¡Œä¸åŒå¤„ç†
            if file_obj.file_type == FileType.DOCUMENT:
                result = _process_document_file(file_obj, db)
            elif file_obj.file_type == FileType.IMAGE:
                result = _process_image_file(file_obj, db)
            elif file_obj.file_type == FileType.VIDEO:
                result = _process_video_file(file_obj, db)
            elif file_obj.file_type == FileType.TEXT:
                result = _process_text_file(file_obj, db)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_obj.file_type}")
            
            # æ›´æ–°æ–‡ä»¶å¤„ç†å®ŒæˆçŠ¶æ€
            file_service.update_file_status(file_id, FileStatus.COMPLETED, progress=100)
            
            # æ›´æ–°æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
            file_service.update_file_violation_count(file_id)
            
            # æ›´æ–°ä»»åŠ¡è¿›åº¦
            _update_task_progress(task_id, db)
            
            logger.info(f"âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {file_obj.original_name}, å‘ç°{len(result)}ä¸ªæ£€æµ‹ç»“æœ")
            
            return {
                "status": "completed",
                "file_id": file_id,
                "results_count": len(result),
                "message": "æ–‡ä»¶å¤„ç†å®Œæˆ"
            }
    
    except RuntimeError as e:
        # é”å†²çªï¼Œæ–‡ä»¶å·²è¢«å…¶ä»–è¿›ç¨‹å¤„ç†
        logger.warning(f"æ–‡ä»¶é”å†²çª: {e}")
        return {"status": "skipped", "reason": "æ–‡ä»¶æ­£åœ¨è¢«å…¶ä»–è¿›ç¨‹å¤„ç†"}
    
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ {file_id}: {e}")
        
        try:
            file_service = FileService(db)
            file_service.update_file_status(
                file_id, 
                FileStatus.FAILED, 
                error_message=str(e)
            )
            _update_task_progress(task_id, db)
        except Exception as update_error:
            logger.error(f"æ›´æ–°æ–‡ä»¶å¤±è´¥çŠ¶æ€æ—¶å‡ºé”™: {update_error}")
        
        return {"status": "failed", "file_id": file_id, "error": str(e)}
    
    finally:
        db.close()


def _process_document_file(file_obj, db: Session) -> List[Dict]:
    """å¤„ç†æ–‡æ¡£æ–‡ä»¶ï¼ˆä¿®å¤ç‰ˆï¼‰"""
    try:
        # è·å–ä»»åŠ¡ä¿¡æ¯
        task = db.query(ReviewTask).filter(ReviewTask.id == file_obj.task_id).first()
        if not task:
            logger.error(f"æ— æ³•æ‰¾åˆ°ä»»åŠ¡: {file_obj.task_id}")
            return []
        
        strategy_type = task.strategy_type
        strategy_contents = task.strategy_contents
        
        logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {file_obj.original_name}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_obj.file_path):
            logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_obj.file_path}")
            return []
        
        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©å¤„ç†æ–¹å¼
        file_ext = os.path.splitext(file_obj.original_name)[1].lower()
        
        if file_ext == '.pdf':
            return _process_pdf_file(file_obj, strategy_type, strategy_contents, db)
        elif file_ext in ['.docx', '.doc']:
            return _process_word_file(file_obj, strategy_type, strategy_contents, db)
        elif file_ext == '.txt':
            return _process_text_file(file_obj, db)
        else:
            logger.warning(f"ä¸æ”¯æŒçš„æ–‡æ¡£ç±»å‹: {file_ext}")
            return []
    
    except Exception as e:
        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}", exc_info=True)
        return []


def _process_pdf_file(file_obj, strategy_type: str, strategy_contents: str, db: Session) -> List[Dict]:
    """å¤„ç†PDFæ–‡ä»¶ - ä¿®å¤é¡µé¢é™åˆ¶"""
    try:
        
        logger.info(f"å¤„ç†PDFæ–‡ä»¶: {file_obj.original_name}")
        
        all_violations = []
        has_extractable_text = False
        
        # æ–¹æ³•1ï¼šå°è¯•æå–æ–‡æœ¬å†…å®¹
        try:
            with open(file_obj.file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text_content = ""
                text_length_threshold = 100  # å¦‚æœæå–çš„æ–‡æœ¬å°‘äº100å­—ç¬¦ï¼Œè®¤ä¸ºæ˜¯æ‰«æç‰ˆ
                
                for page_num, page in enumerate(pdf_reader.pages, 1):
                    page_text = page.extract_text()
                    if page_text.strip():
                        text_content += f"\n[é¡µé¢ {page_num}]\n{page_text}"
                
                # åˆ¤æ–­æ˜¯å¦æœ‰è¶³å¤Ÿçš„å¯æå–æ–‡æœ¬
                if len(text_content.strip()) > text_length_threshold:
                    has_extractable_text = True
                    logger.info(f"ä»PDFæå–åˆ°æ–‡æœ¬ï¼Œé•¿åº¦: {len(text_content)} (æ–‡æœ¬ç‰ˆPDF)")
                    
                    # å®¡æ ¸æå–çš„æ–‡æœ¬
                    text_violations = _review_text_content_sync(
                        text_content, strategy_type, strategy_contents
                    )
                    for violation in text_violations:
                        violation["source_description"] = "PDFæ–‡æœ¬æå–"
                        _save_violation_result(violation, str(file_obj.id), None, db)
                    all_violations.extend(text_violations)
                else:
                    logger.info("PDFæ–‡æœ¬æå–é‡è¾ƒå°‘ï¼Œå¯èƒ½æ˜¯æ‰«æç‰ˆPDF")
        
        except Exception as e:
            logger.warning(f"PDFæ–‡æœ¬æå–å¤±è´¥: {e}")
        
        # æ–¹æ³•2ï¼šå›¾ç‰‡OCRå¤„ç† - ç§»é™¤é¡µé¢é™åˆ¶
        should_do_ocr = True
        
        if has_extractable_text:
            # å¦‚æœå·²ç»æœ‰æ–‡æœ¬å†…å®¹ï¼Œä»ç„¶å¯¹æ‰€æœ‰é¡µé¢è¿›è¡Œå›¾åƒå®¡æ ¸ï¼ˆæ£€æŸ¥å›¾åƒè¿è§„ï¼‰
            logger.info("PDFæœ‰å¯æå–æ–‡æœ¬ï¼Œå°†å¯¹æ‰€æœ‰é¡µé¢è¿›è¡Œå›¾åƒå†…å®¹å®¡æ ¸")
        else:
            # å¦‚æœæ²¡æœ‰æ–‡æœ¬å†…å®¹ï¼Œå¯¹æ‰€æœ‰é¡µé¢åšOCR
            logger.info("PDFæ–‡æœ¬æå–è¾ƒå°‘ï¼Œå°†å¯¹æ‰€æœ‰é¡µé¢è¿›è¡ŒOCRå’Œå›¾åƒå®¡æ ¸")
        
        if should_do_ocr:
            try:
                logger.info(f"å°†PDFè½¬æ¢ä¸ºå›¾ç‰‡è¿›è¡Œå¤„ç†ï¼ˆæ‰€æœ‰é¡µé¢ï¼‰")
                # ç§»é™¤ last_page å‚æ•°ï¼Œå¤„ç†æ‰€æœ‰é¡µé¢
                images = convert_from_path(file_obj.file_path, dpi=200)
                
                for page_num, image in enumerate(images, 1):
                    # ä¿å­˜ä¸´æ—¶å›¾ç‰‡
                    temp_image_path = f"/tmp/pdf_page_{file_obj.id}_{page_num}.jpg"
                    image.save(temp_image_path, 'JPEG')
                    # å¤åˆ¶åˆ°é™æ€ç›®å½•
                    static_url = _copy_to_static(temp_image_path, f"pdf_page_{file_obj.id}_{page_num}")
                    
                    try:
                        # OCR + è§†è§‰å®¡æ ¸
                        page_violations = _process_image_content_sync(
                            temp_image_path, strategy_type, strategy_contents, page_num
                        )
                        
                        # æ·»åŠ é¡µé¢æ¥æºä¿¡æ¯å¹¶ä¿å­˜
                        for violation in page_violations:
                            violation["source_description"] = f"PDFç¬¬{page_num}é¡µå›¾åƒ"
                            if static_url:
                                if not violation.get("position"):
                                    violation["position"] = {}
                                violation["position"]["static_url"] = static_url
                            _save_violation_result(violation, str(file_obj.id), page_num, db)
                        all_violations.extend(page_violations)
                        
                    finally:
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        if os.path.exists(temp_image_path):
                            os.remove(temp_image_path)
            
            except Exception as e:
                logger.warning(f"PDFå›¾ç‰‡è½¬æ¢å¤±è´¥: {e}")
        
        # ç¡®ä¿æ•°æ®åº“æäº¤
        try:
            db.commit()
            logger.info(f"PDFå¤„ç†å®Œæˆï¼Œä¿å­˜äº† {len(all_violations)} ä¸ªæ£€æµ‹ç»“æœåˆ°æ•°æ®åº“")
        except Exception as commit_error:
            logger.error(f"æäº¤æ•°æ®åº“å¤±è´¥: {commit_error}")
            db.rollback()
        
        # æ›´æ–°OCRç»Ÿè®¡
        _update_file_ocr_stats(file_obj, len(all_violations), db)
        
        processing_method = "æ–‡æœ¬æå–" if has_extractable_text else "OCRè¯†åˆ«"
        logger.info(f"PDFå¤„ç†å®Œæˆ({processing_method})ï¼Œå‘ç° {len(all_violations)} ä¸ªæ£€æµ‹ç»“æœ")
        return all_violations
    
    except Exception as e:
        logger.error(f"PDFå¤„ç†å¤±è´¥: {e}", exc_info=True)
        db.rollback()
        return []


# ä¿®æ”¹ _process_word_file å‡½æ•° - ç¡®ä¿æ•°æ®åº“æäº¤
def _process_word_file(file_obj, strategy_type: str, strategy_contents: str, db: Session) -> List[Dict]:
    """å¤„ç†Wordæ–‡ä»¶"""
    try:
        
        logger.info(f"å¤„ç†Wordæ–‡ä»¶: {file_obj.original_name}")
        
        # æå–Wordæ–‡æ¡£æ–‡æœ¬
        doc = Document(file_obj.file_path)
        text_content = ""
        
        for para in doc.paragraphs:
            if para.text.strip():
                text_content += para.text + "\n"
        
        if not text_content.strip():
            logger.warning("Wordæ–‡æ¡£ä¸­æ²¡æœ‰æå–åˆ°æ–‡æœ¬å†…å®¹")
            return []
        
        logger.info(f"ä»Wordæå–åˆ°æ–‡æœ¬ï¼Œé•¿åº¦: {len(text_content)}")
        
        # å®¡æ ¸æ–‡æœ¬å†…å®¹
        violations = _review_text_content_sync(text_content, strategy_type, strategy_contents)
        
        # æ·»åŠ ä¿å­˜é€»è¾‘
        for violation in violations:
            _save_violation_result(violation, str(file_obj.id), 1, db)
        
        # ç¡®ä¿æ•°æ®åº“æäº¤
        try:
            db.commit()
            logger.info(f"Wordå¤„ç†å®Œæˆï¼Œä¿å­˜äº† {len(violations)} ä¸ªæ£€æµ‹ç»“æœåˆ°æ•°æ®åº“")
        except Exception as commit_error:
            logger.error(f"æäº¤æ•°æ®åº“å¤±è´¥: {commit_error}")
            db.rollback()
        
        # æ›´æ–°ç»Ÿè®¡
        _update_file_ocr_stats(file_obj, len(violations), db)
        
        logger.info(f"Wordå¤„ç†å®Œæˆï¼Œå‘ç° {len(violations)} ä¸ªæ£€æµ‹ç»“æœ")
        return violations
    
    except Exception as e:
        logger.error(f"Wordæ–‡ä»¶å¤„ç†å¤±è´¥: {e}", exc_info=True)
        db.rollback()
        return []


# ä¿®æ”¹ _process_text_file å‡½æ•° - ç¡®ä¿æ•°æ®åº“æäº¤
def _process_text_file(file_obj, db: Session) -> List[Dict]:
    """å¤„ç†çº¯æ–‡æœ¬æ–‡ä»¶ï¼ˆä¿®å¤ç‰ˆï¼‰"""
    try:
        # è·å–ä»»åŠ¡ä¿¡æ¯
        task = db.query(ReviewTask).filter(ReviewTask.id == file_obj.task_id).first()
        if not task:
            logger.error(f"æ— æ³•æ‰¾åˆ°ä»»åŠ¡: {file_obj.task_id}")
            return []
        
        strategy_type = task.strategy_type
        strategy_contents = task.strategy_contents
        
        logger.info(f"å¼€å§‹å¤„ç†æ–‡æœ¬æ–‡ä»¶: {file_obj.original_name}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_obj.file_path):
            logger.error(f"æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {file_obj.file_path}")
            return []
        
        # è¯»å–æ–‡æœ¬å†…å®¹
        text_content = ""
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin1']
        
        for encoding in encodings:
            try:
                with open(file_obj.file_path, 'r', encoding=encoding) as f:
                    text_content = f.read()
                logger.info(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶")
                break
            except UnicodeDecodeError:
                continue
        
        if not text_content:
            logger.error("æ— æ³•è¯»å–æ–‡æœ¬æ–‡ä»¶å†…å®¹")
            return []
        
        if not text_content.strip():
            logger.warning("æ–‡æœ¬æ–‡ä»¶å†…å®¹ä¸ºç©º")
            return []
        
        logger.info(f"è¯»å–åˆ°æ–‡æœ¬å†…å®¹ï¼Œé•¿åº¦: {len(text_content)}")
        
        # å®¡æ ¸æ–‡æœ¬å†…å®¹
        violations = _review_text_content_sync(text_content, strategy_type, strategy_contents)
        
        # æ·»åŠ ä¿å­˜é€»è¾‘
        for violation in violations:
            _save_violation_result(violation, str(file_obj.id), 1, db)
        
        # ç¡®ä¿æ•°æ®åº“æäº¤
        try:
            db.commit()
            logger.info(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å®Œæˆï¼Œä¿å­˜äº† {len(violations)} ä¸ªæ£€æµ‹ç»“æœåˆ°æ•°æ®åº“")
        except Exception as commit_error:
            logger.error(f"æäº¤æ•°æ®åº“å¤±è´¥: {commit_error}")
            db.rollback()
        
        # æ›´æ–°ç»Ÿè®¡
        _update_file_ocr_stats(file_obj, len(violations), db)
        
        logger.info(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å®Œæˆï¼Œå‘ç° {len(violations)} ä¸ªæ£€æµ‹ç»“æœ")
        return violations
    
    except Exception as e:
        logger.error(f"æ–‡æœ¬æ–‡ä»¶å¤„ç†å¤±è´¥: {e}", exc_info=True)
        db.rollback()
        return []

def _process_video_file(file_obj, db: Session) -> List[Dict]:
    """å¤„ç†è§†é¢‘æ–‡ä»¶ - ç¡®ä¿å®¡æ ¸ç»“æœä¸è§†é¢‘å¸§ä¸€ä¸€å¯¹åº”"""
    try:
        # è·å–ä»»åŠ¡ä¿¡æ¯
        task = db.query(ReviewTask).filter(ReviewTask.id == file_obj.task_id).first()
        if not task:
            logger.error(f"æ— æ³•æ‰¾åˆ°ä»»åŠ¡: {file_obj.task_id}")
            return []
        
        strategy_type = task.strategy_type
        strategy_contents = task.strategy_contents
        frame_interval = task.video_frame_interval or 5
        
        logger.info(f"å¼€å§‹å¤„ç†è§†é¢‘: {file_obj.original_name}, æŠ½å¸§é—´éš”: {frame_interval}ç§’")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_obj.file_path):
            logger.error(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {file_obj.file_path}")
            return []
        
        # æå–è§†é¢‘å¸§
        frame_paths = _extract_video_frames_with_metadata(file_obj.file_path, frame_interval, str(file_obj.id))
        
        if not frame_paths:
            logger.warning("æ²¡æœ‰æˆåŠŸæå–åˆ°è§†é¢‘å¸§")
            return []
        
        logger.info(f"æˆåŠŸæå– {len(frame_paths)} ä¸ªè§†é¢‘å¸§")
        
        all_violations = []
        
        # å¤„ç†æ¯ä¸€å¸§ - å»ºç«‹ç²¾ç¡®å¯¹åº”å…³ç³»
        for frame_info in frame_paths:
            frame_path = frame_info["path"]
            frame_number = frame_info["frame_number"]
            timestamp = frame_info["timestamp"]
            relative_path = frame_info["relative_path"]
            
            logger.info(f"å¤„ç†ç¬¬ {frame_number} å¸§, æ—¶é—´: {timestamp}s, è·¯å¾„: {frame_path}")
            
            # å¤„ç†å•å¸§
            frame_violations = _process_image_content_sync(
                frame_path, strategy_type, strategy_contents
            )
            
            # ä¸ºæ¯ä¸ªè¿è§„ç»“æœå»ºç«‹ç²¾ç¡®çš„å¸§å¯¹åº”å…³ç³»
            for violation in frame_violations:
                # æ„å»ºå®Œæ•´çš„å¸§ä¿¡æ¯
                frame_metadata = {
                    "timestamp": timestamp,
                    "frame_number": frame_number,
                    "frame_path": frame_path,
                    "relative_path": relative_path,  # ç”¨äºAPIè®¿é—®
                    "file_id": str(file_obj.id),
                    "original_video": file_obj.original_name
                }
                
                # å°†å¸§ä¿¡æ¯æ·»åŠ åˆ°è¿è§„ç»“æœä¸­
                violation["timestamp"] = timestamp
                violation["position"] = frame_metadata
                violation["frame_metadata"] = frame_metadata  # é¢å¤–çš„å…ƒæ•°æ®å­—æ®µ
                
                # ä¿å­˜åˆ°æ•°æ®åº“æ—¶åŒ…å«å®Œæ•´çš„å¸§ä¿¡æ¯
                _save_violation_result_with_frame_info(
                    violation, 
                    str(file_obj.id), 
                    frame_number, 
                    db, 
                    timestamp,
                    frame_metadata
                )
                all_violations.append(violation)
        
        # ç¡®ä¿æ•°æ®åº“æäº¤
        try:
            db.commit()
            logger.info(f"è§†é¢‘å¤„ç†å®Œæˆï¼Œä¿å­˜äº† {len(all_violations)} ä¸ªæ£€æµ‹ç»“æœåˆ°æ•°æ®åº“")
        except Exception as commit_error:
            logger.error(f"æäº¤æ•°æ®åº“å¤±è´¥: {commit_error}")
            db.rollback()
        
        # æ›´æ–°ç»Ÿè®¡
        _update_file_ocr_stats(file_obj, len(all_violations), db)
        
        logger.info(f"è§†é¢‘å¤„ç†å®Œæˆï¼Œå‘ç° {len(all_violations)} ä¸ªæ£€æµ‹ç»“æœï¼Œå·²å»ºç«‹å¸§å¯¹åº”å…³ç³»")
        return all_violations
    
    except Exception as e:
        logger.error(f"è§†é¢‘å¤„ç†å¤±è´¥: {e}", exc_info=True)
        db.rollback()
        return []


def _extract_video_frames_with_metadata(video_path: str, interval: int = 5, file_id: str = None) -> List[Dict]:
    """æå–è§†é¢‘å¸§å¹¶è¿”å›è¯¦ç»†å…ƒæ•°æ®"""
    try:
        frame_infos = []
        
        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            return []
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps if fps > 0 else 0
        
        if fps <= 0:
            logger.error("æ— æ³•è·å–è§†é¢‘å¸§ç‡")
            cap.release()
            return []
        
        frame_interval = int(fps * interval)  # é—´éš”å¸§æ•°
        
        # åˆ›å»ºå¸§å­˜å‚¨ç›®å½•
        from app.config import get_settings
        settings = get_settings()
        frames_dir = os.path.join(settings.UPLOAD_DIR, "video_frames", file_id or "unknown")
        os.makedirs(frames_dir, exist_ok=True)
        
        frame_num = 0
        saved_frames = 0
        max_frames = int(duration / interval)
        logger.info(f"è§†é¢‘ä¿¡æ¯: FPS={fps}, æ€»å¸§æ•°={total_frames}, æ—¶é•¿={duration:.1f}s")
        logger.info(f"æå–å‚æ•°: é—´éš”={interval}s({frame_interval}å¸§), æœ€å¤§å¸§æ•°={max_frames}")
        
        while saved_frames < max_frames and frame_num < total_frames:
            ret, frame = cap.read()
            if not ret:
                break
            
            # æŒ‰é—´éš”ä¿å­˜å¸§
            if frame_num % frame_interval == 0:
                frame_time = frame_num / fps
                
                # ç”Ÿæˆæ ‡å‡†åŒ–çš„æ–‡ä»¶å
                frame_filename = f"frame_{saved_frames+1:04d}_time_{frame_time:.1f}s_pos_{frame_num:06d}.jpg"
                frame_path = os.path.join(frames_dir, frame_filename)
                
                if cv2.imwrite(frame_path, frame):
                    # å¤åˆ¶åˆ°é™æ€ç›®å½•
                    static_url = _copy_to_static(frame_path, f"frame_{file_id}_{saved_frames+1:04d}")
                    # æ„å»ºå¸§ä¿¡æ¯å¯¹è±¡
                    frame_info = {
                        "frame_number": saved_frames + 1,  # ä»1å¼€å§‹çš„å¸§åºå·
                        "video_frame_position": frame_num,  # åœ¨è§†é¢‘ä¸­çš„å®é™…å¸§ä½ç½®
                        "timestamp": round(frame_time, 1),  # æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
                        "path": frame_path,  # å®Œæ•´è·¯å¾„
                        "static_url": static_url,
                        "filename": frame_filename,  # æ–‡ä»¶å
                        "relative_path": f"video_frames/{file_id}/{frame_filename}",  # ç›¸å¯¹è·¯å¾„
                        "file_size": os.path.getsize(frame_path) if os.path.exists(frame_path) else 0
                    }
                    
                    frame_infos.append(frame_info)
                    saved_frames += 1
                    logger.info(f"ä¿å­˜å¸§ {saved_frames}: {frame_filename} (æ—¶é—´: {frame_time:.1f}s)")
                else:
                    logger.warning(f"ä¿å­˜å¸§å¤±è´¥: {frame_path}")
            
            frame_num += 1
        
        cap.release()
        
        logger.info(f"è§†é¢‘å¸§æå–å®Œæˆï¼Œå…±æå– {len(frame_infos)} å¸§")
        return frame_infos
    
    except Exception as e:
        logger.error(f"è§†é¢‘å¸§æå–å¤±è´¥: {e}", exc_info=True)
        return []


def _save_violation_result_with_frame_info(
    violation: Dict, 
    file_id: str, 
    frame_number: int,
    db: Session,
    timestamp: float,
    frame_metadata: Dict
):
    """ä¿å­˜è¿è§„ç»“æœå¹¶å»ºç«‹ä¸å¸§çš„ç²¾ç¡®å¯¹åº”å…³ç³»"""
    try:
        # è·å–æ£€æµ‹ç»“æœ
        violation_result_str = violation.get("violation_result", "ä¸ç¡®å®š")
        violation_result = _get_violation_result_enum(violation_result_str)
        
        # åˆ›å»ºå®¡æ ¸ç»“æœå¯¹è±¡
        result = ReviewResult(
            file_id=file_id,
            violation_result=violation_result,
            source_type=SourceType(violation.get("source_type", "visual")),
            confidence_score=float(violation.get("confidence_score", 0.0)),
            evidence=violation.get("evidence", ""),
            evidence_text=violation.get("evidence_text"),
            
            # å…³é”®ï¼šä¿å­˜å®Œæ•´çš„å¸§å…³è”ä¿¡æ¯
            position=frame_metadata,  # å®Œæ•´çš„å¸§å…ƒæ•°æ®
            page_number=frame_number,  # å¸§åºå·
            timestamp=timestamp,  # æ—¶é—´æˆ³
            
            model_name=violation.get("model_name"),
            model_version=violation.get("model_version"),
            raw_response=violation.get("raw_response")
        )
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        db.add(result)
        
        logger.info(f"ä¿å­˜è¿è§„ç»“æœ: å¸§{frame_number}, æ—¶é—´{timestamp}s, ç»“æœ:{violation_result.value}")
        
        return result
    
    except Exception as e:
        logger.error(f"ä¿å­˜è¿è§„ç»“æœå¤±è´¥: {e}")
        if db:
            try:
                db.rollback()
            except Exception as rollback_error:
                logger.error(f"å›æ»šäº‹åŠ¡å¤±è´¥: {rollback_error}")




# ä¿®å¤ _process_image_file å‡½æ•°çš„ä¿å­˜é€»è¾‘
def _process_image_file(file_obj, db: Session) -> List[Dict]:
    """å¤„ç†å›¾ç‰‡æ–‡ä»¶ï¼ˆä¿®å¤ç‰ˆï¼‰"""
    try:
        # ä¸ºä¸Šä¼ çš„å›¾ç‰‡åˆ›å»ºé™æ€é“¾æ¥
        static_url = _copy_to_static(file_obj.file_path, f"upload_{file_obj.id}")
        ocr_service = OCRService()
        ai_service = AIReviewService()
        
        # ä¿®å¤ï¼šæ­£ç¡®è·å–ä»»åŠ¡ä¿¡æ¯
        task = db.query(ReviewTask).filter(ReviewTask.id == file_obj.task_id).first()
        if not task:
            logger.error(f"æ— æ³•æ‰¾åˆ°ä»»åŠ¡: {file_obj.task_id}")
            return []
        
        strategy_type = task.strategy_type
        strategy_contents = task.strategy_contents
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            all_violations = []
            
            # OCRæå–å†…å®¹
            ocr_result = loop.run_until_complete(
                ocr_service.extract_content(file_obj.file_path)
            )
            
            if ocr_result.get("success"):
                # å¤„ç†æ–‡æœ¬å—
                text_blocks = [block for block in ocr_result.get("blocks", []) 
                              if block["type"] == "text"]
                if text_blocks:
                    text_content = " ".join([block["text"] for block in text_blocks])
                    text_violations = loop.run_until_complete(
                        ai_service.review_text_content(text_content, strategy_type, strategy_contents)
                    )

                    for violation in text_violations:
                        if static_url and not violation.get("position"):
                            violation["position"] = {"static_url": static_url}
                        _save_violation_result(violation, str(file_obj.id), 1, db)
                        all_violations.append(violation)
            
            # ç›´æ¥å¯¹æ•´ä¸ªå›¾ç‰‡è¿›è¡Œè§†è§‰å®¡æ ¸
            visual_violations = loop.run_until_complete(
                ai_service.review_visual_content(file_obj.file_path, strategy_type, strategy_contents)
            )
            
            for violation in visual_violations:
                if static_url and not violation.get("position"):
                    violation["position"] = {"static_url": static_url}
                _save_violation_result(violation, str(file_obj.id), 1, db)
                all_violations.append(violation)
            
            # å…³é”®ä¿®å¤ï¼šåœ¨å¾ªç¯å¤–è¿›è¡Œæ•°æ®åº“æäº¤
            
        finally:
            loop.close()
        
        # ä¿®å¤ï¼šç¡®ä¿æ•°æ®åº“æäº¤åœ¨ finally å—å¤–é¢
        try:
            db.commit()
            logger.info(f"å›¾ç‰‡å¤„ç†å®Œæˆï¼Œä¿å­˜äº† {len(all_violations)} ä¸ªæ£€æµ‹ç»“æœåˆ°æ•°æ®åº“")
        except Exception as commit_error:
            logger.error(f"æäº¤æ•°æ®åº“å¤±è´¥: {commit_error}")
            db.rollback()
        
        return all_violations
        
    except Exception as e:
        logger.error(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {e}", exc_info=True)
        db.rollback()
        return []


# ä¿®æ”¹ _process_image_content_sync å‡½æ•°ï¼Œè®©å®ƒåªè¿”å›ç»“æœï¼Œä¸ä¿å­˜
def _process_image_content_sync(image_path: str, strategy_type: str, strategy_contents: str, page_num: int = 1) -> List[Dict]:
    """åŒæ­¥å¤„ç†å›¾ç‰‡å†…å®¹ï¼ˆåªè¿”å›ç»“æœï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“ï¼‰"""
    try:
        
        ocr_service = OCRService()
        ai_service = AIReviewService()
        
        # ä½¿ç”¨äº‹ä»¶å¾ªç¯
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            violations = []
            
            # OCRæå–å†…å®¹
            ocr_result = loop.run_until_complete(
                ocr_service.extract_content(image_path)
            )
            
            if ocr_result.get("success"):
                # å¤„ç†æ–‡æœ¬å—
                text_blocks = [block for block in ocr_result.get("blocks", []) 
                              if block["type"] == "text"]
                if text_blocks:
                    text_content = " ".join([block["text"] for block in text_blocks])
                    if text_content.strip():
                        text_violations = loop.run_until_complete(
                            ai_service.review_text_content(text_content, strategy_type, strategy_contents)
                        )
                        
                        for violation in text_violations:
                            violation["page_number"] = page_num
                            violations.append(violation)
            
            # è§†è§‰å†…å®¹å®¡æ ¸
            visual_violations = loop.run_until_complete(
                ai_service.review_visual_content(image_path, strategy_type, strategy_contents)
            )
            
            for violation in visual_violations:
                violation["page_number"] = page_num
                violations.append(violation)
            
            return violations
        
        finally:
            loop.close()
    
    except Exception as e:
        logger.error(f"å›¾ç‰‡å†…å®¹å¤„ç†å¤±è´¥: {e}", exc_info=True)
        return []


# ä¿®æ”¹ _review_text_content_sync å‡½æ•°ï¼Œè®©å®ƒåªè¿”å›ç»“æœï¼Œä¸ä¿å­˜
def _review_text_content_sync(text_content: str, strategy_type: str, strategy_contents: str) -> List[Dict]:
    """åŒæ­¥å®¡æ ¸æ–‡æœ¬å†…å®¹ï¼ˆåªè¿”å›ç»“æœï¼Œä¸ä¿å­˜åˆ°æ•°æ®åº“ï¼‰"""
    try:
        
        ai_service = AIReviewService()
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        
        try:
            violations = loop.run_until_complete(
                ai_service.review_text_content(text_content, strategy_type, strategy_contents)
            )
            return violations
        finally:
            loop.close()
    
    except Exception as e:
        logger.error(f"æ–‡æœ¬å®¡æ ¸å¤±è´¥: {e}", exc_info=True)
        return []


def _update_file_ocr_stats(file_obj, results_count: int, db: Session):
    """æ›´æ–°æ–‡ä»¶OCRç»Ÿè®¡"""
    try:
        
        file_service = FileService(db)
        file_service.update_file_ocr_stats(
            str(file_obj.id), 
            results_count,  # total blocks
            results_count,  # text blocks (ç®€åŒ–)
            0               # image blocks
        )
    except Exception as e:
        logger.warning(f"æ›´æ–°æ–‡ä»¶ç»Ÿè®¡å¤±è´¥: {e}")

def _update_task_progress(task_id: str, db: Session):
    """
    æ›´æ–°ä»»åŠ¡è¿›åº¦ï¼ˆå¸¦é”ä¿æŠ¤ï¼‰
    """
    try:
        
        task_service = TaskService(db)
        
        # ç»Ÿè®¡å·²å®Œæˆçš„æ–‡ä»¶æ•°
        completed_files = db.query(ReviewFile).filter(
            ReviewFile.task_id == task_id,
            ReviewFile.status.in_([FileStatus.COMPLETED, FileStatus.FAILED])
        ).count()
        
        # æ›´æ–°ä»»åŠ¡è¿›åº¦
        task_service.update_task_progress(task_id, completed_files)
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ–‡ä»¶éƒ½å¤„ç†å®Œæˆ
        total_files = db.query(ReviewFile).filter(
            ReviewFile.task_id == task_id
        ).count()
        
        if completed_files >= total_files:
            # è·å–å¤±è´¥æ–‡ä»¶æ•°
            failed_files = db.query(ReviewFile).filter(
                ReviewFile.task_id == task_id,
                ReviewFile.status == FileStatus.FAILED
            ).count()
            
            # æ ¹æ®å¤±è´¥æ–‡ä»¶æ•°å†³å®šä»»åŠ¡çŠ¶æ€
            if failed_files == 0:
                task_service.complete_task(task_id, success=True)
                logger.info(f"âœ… ä»»åŠ¡ {task_id} å…¨éƒ¨å®Œæˆ")
            elif failed_files < total_files:
                task_service.complete_task(task_id, success=True)
                logger.info(f"âš ï¸ ä»»åŠ¡ {task_id} éƒ¨åˆ†å®Œæˆï¼Œ{failed_files}/{total_files} æ–‡ä»¶å¤±è´¥")
            else:
                task_service.complete_task(task_id, success=False, error_message="æ‰€æœ‰æ–‡ä»¶å¤„ç†å¤±è´¥")
                logger.error(f"âŒ ä»»åŠ¡ {task_id} å…¨éƒ¨å¤±è´¥")
    
    except Exception as e:
        logger.error(f"æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")



def _extract_video_frames_fixed(video_path: str, interval: int = 5, max_frames: int = 20) -> List[str]:
    """æå–è§†é¢‘å¸§ï¼ˆä¿®å¤ç‰ˆï¼‰"""
    try:
        
        frame_paths = []
        
        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            logger.error(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
            return []
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps <= 0:
            logger.error("æ— æ³•è·å–è§†é¢‘å¸§ç‡")
            cap.release()
            return []
        
        frame_interval = int(fps * interval)  # é—´éš”å¸§æ•°
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = f"/tmp/video_frames_{uuid.uuid4()}"
        os.makedirs(temp_dir, exist_ok=True)
        
        frame_num = 0
        saved_frames = 0
        
        logger.info(f"å¼€å§‹æå–è§†é¢‘å¸§ï¼ŒFPS: {fps}, é—´éš”: {frame_interval}å¸§")
        
        while saved_frames < max_frames:
            ret, frame = cap.read()
            if not ret:
                break
            
            # æŒ‰é—´éš”ä¿å­˜å¸§
            if frame_num % frame_interval == 0:
                frame_path = os.path.join(temp_dir, f"frame_{saved_frames:04d}.jpg")
                if cv2.imwrite(frame_path, frame):
                    frame_paths.append(frame_path)
                    saved_frames += 1
                    logger.debug(f"ä¿å­˜å¸§ {saved_frames}: {frame_path}")
                else:
                    logger.warning(f"ä¿å­˜å¸§å¤±è´¥: {frame_path}")
            
            frame_num += 1
        
        cap.release()
        
        logger.info(f"è§†é¢‘å¸§æå–å®Œæˆï¼Œå…±æå– {len(frame_paths)} å¸§")
        return frame_paths
    
    except Exception as e:
        logger.error(f"è§†é¢‘å¸§æå–å¤±è´¥: {e}", exc_info=True)
        return []

def _cleanup_temp_files(file_paths: List[str]):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    try:
        for file_path in file_paths:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    logger.debug(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if file_paths:
            temp_dir = os.path.dirname(file_paths[0])
            try:
                if os.path.exists(temp_dir) and os.path.isdir(temp_dir):
                    os.rmdir(temp_dir)
                    logger.debug(f"æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
            except Exception as e:
                logger.warning(f"æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥ {temp_dir}: {e}")
    
    except Exception as e:
        logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶è¿‡ç¨‹å‡ºé”™: {e}")

def _copy_to_static(temp_path, prefix="evidence"):
    """å¤åˆ¶å›¾ç‰‡åˆ°é™æ€ç›®å½•å¹¶è¿”å›URL"""
    try:
        if not os.path.exists(temp_path):
            return None
        
        # ç”Ÿæˆæ–‡ä»¶å
        ext = os.path.splitext(temp_path)[1] or '.jpg'
        filename = f"{prefix}_{uuid.uuid4()}{ext}"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        static_dir = "/app/static/evidence"
        os.makedirs(static_dir, exist_ok=True)
        
        # å¤åˆ¶æ–‡ä»¶
        static_path = os.path.join(static_dir, filename)
        shutil.copy2(temp_path, static_path)
        
        # è¿”å›URL
        return f"/api/v1/static/evidence/{filename}"
    except:
        return None

def _save_violation_result(
    violation: Dict, 
    file_id: str, 
    page_number: int = None,
    db: Session = None,
    timestamp: float = None
):
    """ä¿å­˜æ£€æµ‹ç»“æœåˆ°æ•°æ®åº“ï¼ˆåŒ…æ‹¬åˆè§„ã€ä¸åˆè§„ã€ä¸ç¡®å®šä¸‰ç§ç»“æœï¼‰"""
    try:
        
        # è·å–æ£€æµ‹ç»“æœ
        violation_result_str = violation.get("violation_result", "ä¸ç¡®å®š")
        violation_result = _get_violation_result_enum(violation_result_str)
        
        # ç¡®ä¿æ•°æ®åº“ä¼šè¯å¯ç”¨
        if db is None:
            logger.error("æ•°æ®åº“ä¼šè¯ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜æ£€æµ‹ç»“æœ")
            return
        
        # åˆ›å»ºå®¡æ ¸ç»“æœå¯¹è±¡
        result = ReviewResult(
            file_id=file_id,
            violation_result=violation_result, 
            source_type=SourceType(violation.get("source_type", "ocr")),
            confidence_score=float(violation.get("confidence_score", 0.0)),
            evidence=violation.get("evidence", ""),
            evidence_text=violation.get("evidence_text"),
            position=violation.get("position"),
            page_number=page_number,
            timestamp=timestamp,
            model_name=violation.get("model_name"),
            model_version=violation.get("model_version"),
            raw_response=violation.get("raw_response")
        )
        
        # ä¿å­˜åˆ°æ•°æ®åº“
        db.add(result)
        db.commit()
        logger.info(f"æˆåŠŸä¿å­˜æ£€æµ‹ç»“æœ: {violation_result.value}, ç½®ä¿¡åº¦: {result.confidence_score}")
    
    except Exception as e:
        logger.error(f"ä¿å­˜æ£€æµ‹ç»“æœå¤±è´¥: {e}")
        if db:
            try:
                db.rollback()
            except Exception as rollback_error:
                logger.error(f"å›æ»šäº‹åŠ¡å¤±è´¥: {rollback_error}")

def _get_violation_result_enum(result_str: str) -> 'ViolationResult':
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ£€æµ‹ç»“æœæšä¸¾"""
    
    if not result_str:
        return ViolationResult.UNCERTAIN
        
    result_map = {
        "ä¸åˆè§„": ViolationResult.NON_COMPLIANT,
        "åˆè§„": ViolationResult.COMPLIANT,
        "ä¸ç¡®å®š": ViolationResult.UNCERTAIN,
        # è‹±æ–‡æ˜ å°„
        "non_compliant": ViolationResult.NON_COMPLIANT,
        "compliant": ViolationResult.COMPLIANT,
        "uncertain": ViolationResult.UNCERTAIN,
    }
    
    # è½¬æ¢ä¸ºå°å†™è¿›è¡ŒåŒ¹é…
    result_lower = result_str.lower()
    
    # å…ˆå°è¯•ç›´æ¥åŒ¹é…
    if result_str in result_map:
        return result_map[result_str]
    
    # å†å°è¯•å°å†™åŒ¹é…
    if result_lower in result_map:
        return result_map[result_lower]
    
    # æ¨¡ç³ŠåŒ¹é…
    if "åˆè§„" in result_str or "compliant" in result_lower:
        return ViolationResult.COMPLIANT
    elif "ä¸åˆè§„" in result_str or "è¿è§„" in result_str or "non" in result_lower:
        return ViolationResult.NON_COMPLIANT
    else:
        return ViolationResult.UNCERTAIN


@celery_app.task(name="cleanup_temp_files")
def cleanup_temp_files(file_paths: List[str]):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    FileUtils.cleanup_temp_files(file_paths)
    return {"cleaned": len(file_paths)}
